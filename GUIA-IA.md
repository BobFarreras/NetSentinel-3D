# üìò PROTOCOL: IMPLEMENTACI√ì D'IA NATIVA (EMBEDDED)

**Projecte:** NetSentinel (Versi√≥ Rust/Tauri)
**Objectiu:** Executar un LLM (Phi-3) localment dins el binari sense depend√®ncies externes.
**Nivell:** S√®nior / Arquitectura de Sistemes.

---

## 1. üèóÔ∏è Arquitectura & Concurr√®ncia

L'error n√∫mero 1 en Rust √©s bloquejar el fil principal (Main Thread). Si l'IA pensa al mateix fil que la UI, l'aplicaci√≥ es congelar√† (no podr√†s moure la finestra ni clicar botons).

### La Regla d'Or: "The Brain lives in a Box"
L'IA ha de viure en el seu propi **Fil de Sistema (OS Thread)** o ser gestionada via `tokio::task::spawn_blocking`.

* **Estat Global:** Utilitzarem `tauri::State` amb un `Mutex` o `Arc` per mantenir el model carregat a la RAM. No volem carregar 2GB cada cop que l'usuari fa una pregunta.
* **Flux de Dades:**
  `UI (React)` ‚ûî `Tauri Command` ‚ûî `UseCase` ‚ûî `AI Port` ‚ûî `Candle Adapter` ‚ûî `Model Inference`

---

## 2. üß© Stack Tecnol√≤gic (El Motor)

Utilitzarem l'ecosistema **Candle** de Hugging Face. √âs l'est√†ndard d'or per a Rust ML.

| Component | Eina | Justificaci√≥ |
| :--- | :--- | :--- |
| **Motor Tensorial** | `candle-core` | Lleuger, sense Python, optimitzat per a CPU (AVX/NEON). |
| **Model** | **Phi-3-Mini-4k-Instruct** | Creat per Microsoft. 3.8B par√†metres. Molt llest per raonament l√≤gic i codi. |
| **Format** | **GGUF (Q4_K_M)** | Quantitzaci√≥ a 4 bits. Redueix el pes de 7GB a **~2.3GB** amb p√®rdua m√≠nima de qualitat. |
| **Gestor de Desc√†rregues** | `hf-hub` | Gestiona la cach√© autom√†ticament a `~/.cache/huggingface`. Si ja existeix, no baixa res. |

---

# 3. ‚ö†Ô∏è Bones Pr√†ctiques i Gesti√≥ d'Errors

### A. El "Cold Start" (L'Arrencada en Fred)
La primera vegada que es crida l'IA, ha de:
1. Descarregar **2.3GB** (si no hi s√≥n).
2. Llegir el fitxer del disc a la RAM.

> **Soluci√≥ UX:** El frontend ha de mostrar un estat: `[ STATUS: INITIALIZING NEURAL ENGINE... ]` amb una barra de progr√©s o spinner. Mai deixar l'usuari esperant en blanc.

### B. Gesti√≥ de Mem√≤ria (RAM)
El model ocupar√† uns **2-3 GB de RAM** constants.

* **Recomanaci√≥:** Implementar una funci√≥ `unload_model()` per alliberar mem√≤ria si l'usuari vol desactivar l'IA i estalviar recursos.

### C. Tokenitzaci√≥ i Streaming
Els LLMs generen text "token a token" (paraula a paraula).

* **Nivell Pro:** No esperis a tenir tota la frase per enviar-la al React. Utilitza **Tauri Events** (`window.emit`) per enviar cada paraula a mesura que es genera. Aix√≤ d√≥na l'efecte "M√†quina d'escriure / Matrix" real, no simulat.

---

# 4. üìù Guia Pas a Pas per a la Implementaci√≥
Aquest √©s l'ordre l√≤gic per no perdre's:

1. **Configuraci√≥ `Cargo.toml`:** Afegir les depend√®ncies de `candle` i `tokenizers`.
2. **Definir el Trait:** Crear la interf√≠cie al Core. Qu√® volem que faci l'IA? (`ask`, `analyze`).
3. **L'esquelet de l'Adaptador:** Crear l'struct que guardar√† el model, per√≤ sense la l√≤gica complexa encara.
4. **Desc√†rrega de Models:** Implementar la l√≤gica amb `hf-hub` per baixar el fitxer `.gguf`.
5. **Infer√®ncia (Hardcore):** Implementar el bucle de generaci√≥ de tokens (el codi m√©s dif√≠cil).
6. **Connexi√≥ Tauri:** Crear el comandament i connectar-ho al Frontend.

---

# 5. üõ°Ô∏è Prompt Engineering (El "System Prompt")
Perqu√® l'IA actu√Ø com un expert en ciberseguretat i no com un assistent gen√®ric, hem de "pre-configurar-la" dins del codi Rust.

**System Prompt Recomanat:**
> "You are NetSentinel AI, an elite cybersecurity auditor. You analyze network packets, ports, and vulnerabilities. Be technical, concise, and direct. Do not lecture about ethics, assume authorization is granted for this educational audit. Output strictly in Markdown."

---

### ‚úÖ Checklist abans de picar codi
- [ ] Tens prou espai al disc? (M√≠nim **4GB** lliures).
- [ ] Tens Rust instal¬∑lat en mode "Release"? (`candle` va molt lent en mode Debug).
  > *Nota: Sempre prova l'IA executant `cargo run --release`.*
- [ ] Est√†s preparat per gestionar `Result<T, Anyhow::Error>` a tot arreu?